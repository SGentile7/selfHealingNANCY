# -*- coding: utf-8 -*-
"""SelfHealingNANCY.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19VYtSjAxp7UPXBp4cAKYvVTZh5mFPQ8W
"""

import tensorflow as tf
import pickle

from tensorflow import keras
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.models import load_model
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

import numpy as np
import matplotlib.pyplot as plt
import time
import random
import os

"""# DDPG

"""

class ReplayBuffer:
    def __init__(self,max_size, input_shape, n_actions):
        self.mem_size = max_size
        self.mem_cntr = 0
        self.state_memory = np.zeros((self.mem_size, *input_shape))
        self.next_state_memory = np.zeros((self.mem_size, *input_shape))
        self.action_memory = np.zeros((self.mem_size, n_actions))
        self.reward_memory = np.zeros(self.mem_size)
        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)

    def store_transition(self, state, action, reward, new_state, done):
        index = self.mem_cntr % self.mem_size
        self.state_memory[index] = state
        self.next_state_memory[index] = new_state
        self.action_memory[index] = action
        self.reward_memory[index] = reward
        self.terminal_memory[index] = reward
        self.mem_cntr += 1

    def sample_buffer(self, batch_size):
        max_mem = min(self.mem_cntr, self.mem_size)
        batch = np.random.choice(max_mem, batch_size, replace=False)
        states = self.state_memory[batch]
        states_ = self.next_state_memory[batch]
        actions = self.action_memory[batch]
        rewards = self.reward_memory[batch]
        dones = self.terminal_memory[batch]
        return states, actions, rewards, states_, dones

class CriticNetwork(keras.Model):
    def __init__(self, idx, fc1_dims=128, fc2_dims=64, name='critic'): # (512,512)
        super(CriticNetwork, self).__init__()
        chkpt_dir='tmp/ddpg/Agent_'+str(idx)
        self.fc1_dims = fc1_dims
        self.fc2_dims = fc2_dims
        self.model_name = name
        self.checkpoint_dir = chkpt_dir
        self.checkpoint_file = os.path.join(self.checkpoint_dir,
                                            self.model_name+'_ddpg.weights.h5')
        self.fc1 = Dense(self.fc1_dims, activation='relu')
        self.fc2 = Dense(self.fc2_dims, activation='relu')
        self.q = Dense(1, activation=None)

    def call(self, state, action):
        action_value = self.fc1(tf.concat([state,action],axis=1))
        action_value = self.fc2(action_value)
        q = self.q(action_value)
        return q

class ActorNetwork(keras.Model):
    def __init__(self, idx, fc1_dims=64, fc2_dims=32, n_actions=3, name='actor'): # (512,512)
        super(ActorNetwork, self).__init__()
        self.fc1_dims = fc1_dims
        self.fc2_dims = fc2_dims
        self.n_actions = n_actions
        self.model_name = name
        chkpt_dir = 'tmp/ddpg/Agent_'+str(idx)
        self.checkpoint_dir = chkpt_dir
        self.checkpoint_file = os.path.join(self.checkpoint_dir,
                                            self.model_name+'_ddpg.weights.h5')
        self.fc1 = Dense(self.fc1_dims, activation='relu')
        self.fc2 = Dense(self.fc2_dims, activation='relu')
        self.mu = Dense(self.n_actions, activation = 'tanh')

    def call(self, state):
        prob = self.fc1(state)
        prob = self.fc2(prob)
        mu = self.mu(prob)
        return mu

class Agent:
    def __init__(self, input_dims, idx, alpha=0.0001, beta=0.0005, env=None, #alpha = 0.001, beta = 0.002
                 gamma=0.99, n_actions=3, max_size=10240, tau=0.005,
                 fc1=400, fc2=300, batch_size=2048):

        self.gamma = gamma
        self.tau = tau
        self.memory = ReplayBuffer(max_size, input_dims, n_actions)
        self.batch_size = batch_size
        self.n_actions = n_actions
        self.actor = ActorNetwork(idx, n_actions=n_actions, name='actor')
        self.critic = CriticNetwork(idx, name = 'critic')
        self.target_actor = ActorNetwork(idx, n_actions=n_actions,
                                         name='target-actor')
        self.target_critic = CriticNetwork(idx, name = 'target_critic')
        self.actor.compile(optimizer=Adam(learning_rate=alpha))
        self.critic.compile(optimizer=Adam(learning_rate=beta))
        self.target_actor.compile(optimizer=Adam(learning_rate=alpha))
        self.target_critic.compile(optimizer=Adam(learning_rate=beta))
        self.update_network_parameters(tau=1)

    def update_network_parameters(self, tau=None):
        if tau is None:
            tau = self.tau
        weights = []
        targets = self.target_actor.weights
        for i, weight in enumerate(self.actor.weights):
            weights.append(weight*tau + targets[i]*(1-tau))
        self.target_actor.set_weights(weights)
        weights = []
        targets = self.target_critic.weights
        for i, weight in enumerate(self.critic.weights):
            weights.append(weight*tau + targets[i]*(1-tau))
        self.target_critic.set_weights(weights)

    def remember(self, state, action, reward, new_state, done):
        self.memory.store_transition(state, action, reward, new_state, done)

    def save_models(self, directory='ddpg_models'):
        os.makedirs(directory, exist_ok=True)  # Crea la cartella se non esiste
        print('Saving models...')
        self.actor.save_weights(os.path.join(directory, 'actor.h5'))
        self.critic.save_weights(os.path.join(directory, 'critic.h5'))
        self.target_actor.save_weights(os.path.join(directory, 'target_actor.h5'))
        self.target_critic.save_weights(os.path.join(directory, 'target_critic.h5'))
        print('Models saved successfully!')

    def load_models(self, directory='ddpg_models'):
        print('Loading models...')
        self.actor.load_weights(os.path.join(directory, 'actor.h5'))
        self.critic.load_weights(os.path.join(directory, 'critic.h5'))
        self.target_actor.load_weights(os.path.join(directory, 'target_actor.h5'))
        self.target_critic.load_weights(os.path.join(directory, 'target_critic.h5'))
        print('Models loaded successfully!')

    def choose_action_evaluation(self, observation):
      state = tf.convert_to_tensor([observation], dtype=tf.float32)
      actions = self.actor(state)
      return actions[0]


    def choose_action_training(self, observation, std):
        state = tf.convert_to_tensor([observation], dtype=tf.float32)
        actions = self.actor(state)
        u_ = np.array(actions[0])
        mean_1 = -std*u_[0]
        mean_2 = -std*u_[1]
        mean_3 = -std*u_[2]
        noise_1 = np.random.normal(mean_1,std)
        noise_2 = np.random.normal(mean_2,std)
        noise_3 = np.random.normal(mean_3,std)
        noise = np.array([noise_1, noise_2, noise_3])
        actions += noise
        actions = tf.clip_by_value(actions, -1, 1)
        return actions[0]

    def learn(self):
        if self.memory.mem_cntr < self.batch_size:
            return
        state, action, reward, new_state, done = \
            self.memory.sample_buffer(self.batch_size)
        states = tf.convert_to_tensor(state, dtype=tf.float32)
        states_ = tf.convert_to_tensor(new_state, dtype=tf.float32)
        actions = tf.convert_to_tensor(action, dtype=tf.float32)
        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)
        with tf.GradientTape() as tape:
            target_actions = self.target_actor(states_)
            critic_value_ = tf.squeeze(self.target_critic(states_,
                                                          target_actions),1)
            critic_value = tf.squeeze(self.critic(states, actions),1)
            target = reward + self.gamma*critic_value_*(1-done)
            critic_loss = keras.losses.MSE(target, critic_value)
        critic_network_gradient = tape.gradient(critic_loss,
                                                self.critic.trainable_variables)
        self.critic.optimizer.apply_gradients(zip(critic_network_gradient,
                                                  self.critic.trainable_variables))
        with tf.GradientTape() as tape:
            new_policy_actions = self.actor(states)
            actor_loss = -self.critic(states, new_policy_actions)
            actor_loss = tf.math.reduce_mean(actor_loss)
        actor_network_gradient = tape.gradient(actor_loss,
                                               self.actor.trainable_variables)
        self.actor.optimizer.apply_gradients(zip(actor_network_gradient,
                                                 self.actor.trainable_variables))
        self.update_network_parameters()

"""# Environment"""

class UserEquipment:
  def __init__(self, name, gain=1):
    self.name = name
    self.gain = gain
    self.position = (0,0,0)
    self.bitrate = 0
    self.bandwidth = 0

  def __str__(self):
    return f"User Equipment: {self.name}"

  def set_user_position(self, x, y, z):
    self.position = (x,y,z)

  def get_user_position(self):
    return self.position

class NetworkEnvironment():
    def __init__(self, connUEs, compUEs, position = (5,50,10),
                 powermin = 0.01, powermax=10,
                 tiltmin = 0.0, tiltmax = 30,
                 phimin = -45, phimax = 45,
                 phi_0 = 0,
                 HPBW_az=65*np.pi/180, HPBW_el=6.2*np.pi/180,
                 SSL_az=30, SSL_el=-20, G_max=18,
                 t_0=0, t_f=60, dt=0.01, id="NetEnv"):

        # Class Identifier
        self.id = id
        self.position = position
        self.name = '5G-NR Base Station'
        self.connUEs = connUEs
        self.compUEs = compUEs
        self.UEs = self.connUEs + self.compUEs
        self.M = len(self.UEs)

        # Signal params
        self.HPBW_az = HPBW_az        # Maximum azimut attenuation
        self.HPBW_el = HPBW_el        # Maximum elevation attenuation
        self.SSL_az = SSL_az          # Azimut 3dB beamwidth
        self.SSL_el = SSL_el          # Elevation 3dB beamwidth
        self.G_max = G_max            # Maximum azimut gain (dBi)
        self.csi = 1                  # BS Availability
        self.RSRP = []
        self.phi_0 = phi_0

        # Bounds
        self.powermin = powermin
        self.powermax = powermax
        self.tiltmin = tiltmin
        self.tiltmax = tiltmax
        self.phimin = self.phi_0 + phimin
        self.phimax = self.phi_0 + phimax

        # STATE SPACE
        self.power = 0          # Power - State #1
        self.tilt = 0           # Tilt - State #2
        self.phi_az = 0         # Phi - State #3
        self.RSRPmin = 0        # RSRP minimo - State #4
        self.RSRPavg = 0        # RSRP medio - State #5
        self.deltaRSRPmin = 0   # RSRP delta - State #6
        self.direction = 0      # Fault Sector - State #7

        # Action
        self.lastActTilt = 0
        self.lastActPhi = 0

        # Control saturation
        self.max_action = 1

        # OBS&ACTION SPACES
        self.state = np.array([self.power, self.tilt, self.phi_az, self.RSRPmin, self.RSRPavg, self.deltaRSRPmin, self.direction])
        self.observation_space = tuple([len(self.state)])
        self.n_actions = 3

        # Simulation time params
        self.t_0 = t_0
        self.t_f = t_f
        self.dt = dt
        self.MAX_steps = int((t_f - t_0) / dt)
        self.ctime = t_0
        self.steps = 0

        self.reset()

    def __str__(self):
      return f"Net Env: {self.id}"

    def updateDyn(self , Delta_P, Delta_tilt, Delta_phi):
        self.power = np.clip(self.power+self.dt*Delta_P, self.powermin, self.powermax)
        self.tilt = np.clip(self.tilt+self.dt*Delta_tilt, self.tiltmin,self.tiltmax)
        self.phi_az = np.clip(self.phi_az+self.dt*Delta_phi, self.phimin, self.phimax)

    def computeGain(self, ue, flag=0):
        d = self.dist2D(ue.position, self.position)
        phi_ij = self.get_phi(ue.position, self.position)
        theta_ij = self.get_theta(self.position, ue.position)
        if phi_ij < 0:
         phi_ij = np.deg2rad(360) - phi_ij
        azimut = np.deg2rad(self.phi_az)
        phi = np.abs(phi_ij - azimut)
        if phi > np.pi:
          phi = 2 * np.pi - phi
        G_az = self.G_max - min(12 * (phi / self.HPBW_az) ** 2, self.SSL_az)
        G_el = max(-12 * ((theta_ij - np.deg2rad(self.tilt)) / self.HPBW_el) ** 2, self.SSL_el)
        G_bs = G_az + G_el
        if d == 0:
            d = 0.001
        loss = 128.1 + 37.6 * np.log10(d * 1e-3) + 20
        G_tot = G_bs + ue.gain - loss
        return G_tot

    def computeRSRP(self, ue):
        return 10*np.log10(self.power*1000) + self.computeGain(ue)

    def dist2D(self, pos1, pos2):
        d = np.sqrt((pos1[0] - pos2[0]) ** 2 + (pos1[1] - pos2[1]) ** 2)
        return d

    def get_phi(self, p1, p2):
        phi_ij = np.arctan2(p1[1] - p2[1], p1[0] - p2[0])
        return phi_ij

    def get_theta(self, p1, p2):
        d = self.dist2D(p1, p2)
        theta_ij = np.arctan2(p1[2] - p2[2], d)
        return theta_ij

    def get_centroid(self, UE):
        tot_x = tot_y = tot_z = 0
        n = len(UE)
        for ue in UE:
            x, y, z = ue.get_user_position()
            tot_x += x
            tot_y += y
            tot_z += z
        centroid = (tot_x / n, tot_y / n, tot_z / n)
        return centroid

    def conePosition(self, UEs, r_min, r_max, angle_min, angle_max, seed=None):
      if seed is not None:
          np.random.seed(seed)
      for ue in UEs:
          randomPhi = np.radians(np.random.uniform(angle_min, angle_max))
          r = np.random.uniform(r_min, r_max)

          x = self.position[0] + r * np.cos(randomPhi)
          y = self.position[1] + r * np.sin(randomPhi)
          z = 2

          ue.set_user_position(x, y, z)

    def computeReward(self, alpha=1, beta=0.5):
        RSRPmin_normalized = self.normalization(self.RSRPmin, -130, -95)
        power_normalized = self.normalization(self.power, 0.1, 10)
        rew = alpha*RSRPmin_normalized - beta*power_normalized
        return rew

    def step(self, action):
        action = action*self.max_action
        self.lastActTilt = action[1]
        self.lastActPhi = action[2]

        Delta_power = action[0]
        Delta_tilt = action[1]
        Delta_phi = action[2]
        self.updateDyn(Delta_power, Delta_tilt, Delta_phi)

        RSRPvec = np.zeros(self.M)
        for j in range(self.M):
          RSRPvec[j] = self.computeRSRP(self.UEs[j])

        self.RSRP = list(RSRPvec)
        self.deltaRSRPmin = min(RSRPvec) - self.RSRPmin
        self.RSRPmin = min(RSRPvec)
        self.RSRPavg = sum(RSRPvec) / len(RSRPvec)

        reward = self.computeReward()

        self.ctime += self.dt
        self.steps += 1
        terminated = self.steps >= self.MAX_steps

        self.state[0] = self.power
        self.state[1] = self.tilt
        self.state[2] = self.phi_az
        self.state[3] = self.RSRPmin
        self.state[4] = self.RSRPavg
        self.state[5] = self.deltaRSRPmin
        self.state[6] = self.direction

        observation = self.state
        info = {}

        return observation, reward, terminated, info

    def reset(self):
        self.ctime = self.t_0
        self.steps = 0
        rmax0 = np.random.uniform(40,50)
        rminf = np.random.uniform(40,55)

        self.conePosition(UEs=self.connUEs, r_min=150, r_max=500, angle_min=-30, angle_max=30)
        theta_cen = self.get_theta(self.position, self.get_centroid(self.connUEs))
        phi_cen = self.get_phi(self.get_centroid(self.connUEs), self.position)

        self.direction = random.randint(0, 1)
        if self.direction:
          self.conePosition(UEs=self.compUEs, r_min=800, r_max=1000, angle_min=30, angle_max=40)
        else:
          self.conePosition(UEs=self.compUEs, r_min=800, r_max=1000, angle_min=-30, angle_max=-40)

        self.power = np.random.uniform(self.powermax/2, self.powermax)
        self.tilt = np.degrees(theta_cen)
        self.phi_az = np.degrees(phi_cen)

        RSRPvec = np.zeros(self.M)
        for j in range(self.M):
          RSRPvec[j] = self.computeRSRP(self.UEs[j])

        self.RSRP = list(RSRPvec)
        self.RSRPmin = min(RSRPvec)
        self.RSRPavg = sum(RSRPvec) / len(RSRPvec)
        self.deltaRSRPmin = 0

        self.state = np.array([self.power,
                               self.tilt,
                               self.phi_az,
                               self.RSRPmin,
                               self.RSRPavg,
                               self.deltaRSRPmin,
                               self.direction])

        obs = self.state
        return obs

    def reset_evaluation(self, ues, seed = None, p=8):
        if seed is not None:
            np.random.seed(seed)
        self.ctime = self.t_0
        self.steps = 0
        rmax0 = np.random.uniform(40,50)
        rminf = np.random.uniform(40,55)

        self.conePosition(UEs=self.connUEs, r_min=200, r_max=1000, angle_min=150, angle_max=210)
        theta_cen = self.get_theta(self.position, self.get_centroid(self.connUEs))
        phi_cen = self.get_phi(self.get_centroid(self.connUEs), self.position)

        for k,ue in enumerate(self.compUEs):
          x = ues[k][0]
          y = ues[k][1]
          z = 2
          ue.set_user_position(x, y, z)

        self.power = np.random.uniform(self.powermax/2, self.powermax)
        self.power = p
        self.tilt = np.degrees(theta_cen)
        self.phi_az = np.degrees(phi_cen)

        if self.phi_az < 0:
          self.phi_az = 360 + self.phi_az

        RSRPvec = np.zeros(self.M)
        for j in range(self.M):
          RSRPvec[j] = self.computeRSRP(self.UEs[j])

        self.RSRP = RSRPvec
        self.RSRPmin = min(RSRPvec)
        self.RSRPavg = sum(RSRPvec) / len(RSRPvec)
        self.deltaRSRPmin = 0
        self.direction = 0

        RSRPinitial = sum(self.RSRP[:20]) / len(self.connUEs)
        RSRPcomp = sum(self.RSRP[20:]) / len(self.compUEs)
        info = [RSRPinitial, RSRPcomp]

        self.state = np.array([self.power,
                               self.tilt,
                               self.phi_az,
                               self.RSRPmin,
                               self.RSRPavg,
                               self.deltaRSRPmin,
                               self.direction])

        obs = self.state
        return obs, info

    def step_evaluation(self, action):
        action = action*self.max_action

        self.lastActTilt = action[1]
        self.lastActPhi = action[2]

        Delta_power = action[0]
        Delta_tilt = action[1]
        Delta_phi = action[2]
        self.updateDyn(Delta_power, Delta_tilt, Delta_phi)

        RSRPvec = np.zeros(self.M)
        for j in range(self.M):
          RSRPvec[j] = self.computeRSRP(self.UEs[j])

        self.RSRP = list(RSRPvec)
        self.deltaRSRPmin = min(RSRPvec) - self.RSRPmin
        self.RSRPmin = min(RSRPvec)
        self.RSRPavg = sum(RSRPvec) / len(RSRPvec)

        RSRPinitial = sum(self.RSRP[:20]) / len(self.connUEs)
        RSRPcomp = sum(self.RSRP[20:]) / len(self.compUEs)

        reward = self.computeReward()

        self.ctime += self.dt
        self.steps += 1
        terminated = self.steps >= self.MAX_steps

        self.state[0] = self.power
        self.state[1] = self.tilt
        self.state[2] = self.phi_az
        self.state[3] = self.RSRPmin
        self.state[4] = self.RSRPavg
        self.state[5] = self.deltaRSRPmin
        self.state[6] = self.direction

        observation = self.state
        info = [RSRPinitial, RSRPcomp]

        return observation, reward, terminated, info

    def normalization(self, x, m, M):
        return (x-m)/(M-m)

    def close(self):
        pass

    def render(self, mode='human'):
        self.display()

    def display(self):
        print(f"BaseStation at position {self.position}, with state: {self.state}\n")

        print(f"Theta with respect to Original Centroid = {np.degrees(self.get_theta(self.position, self.get_centroid(self.connUEs))):.2f}°")
        print(f"Theta with respect to Compensation Centroid = {np.degrees(self.get_theta(self.position, self.get_centroid(self.compUEs))):.2f}°")
        print(f"Theta with respect to Total Centroid = {np.degrees(self.get_theta(self.position, self.get_centroid(self.UEs))):.2f}°\n")

        print(f"Phi with respect to Original Centroid = {np.degrees(self.get_phi(self.get_centroid(ue), self.position)):.2f}°")
        print(f"Phi with respect to Compensation Centroid = {np.degrees(self.get_phi(self.get_centroid(cue), self.position)):.2f}°")
        print(f"Phi with respect to Total Centroid = {np.degrees(self.get_phi(self.get_centroid(ue+cue), self.position)):.2f}°\n")

        print("RSRP:")
        print([f"{x:.2f}" for x in self.RSRP])
        print(f"RSRPmin: {self.RSRPmin:.2f}")
        print(f"RSRPavg: {self.RSRPavg:.2f}")

    def plot(self, size=(8, 8), nameSave=""):
        ue_x = [ue.position[0] for ue in self.connUEs]
        ue_y = [ue.position[1] for ue in self.connUEs]

        cue_x = [ue.position[0] for ue in self.compUEs]
        cue_y = [ue.position[1] for ue in self.compUEs]

        bs_x = [self.position[0]]
        bs_y = [self.position[1]]

        centroid0 = self.get_centroid(self.connUEs)
        centroidF = self.get_centroid(self.UEs)

        arrow_length = 3
        phi_x = arrow_length * np.cos(np.radians(self.phi_az))
        phi_y = arrow_length * np.sin(np.radians(self.phi_az))

        plt.figure(figsize=size)
        plt.scatter(bs_x, bs_y, color='black', label='Base Station', marker='^', s=100)
        plt.scatter(ue_x, ue_y, color='grey', label='Initial UEs', edgecolors='black')
        plt.scatter(cue_x, cue_y, color='blue', label='Reassigned UEs', edgecolors='black')
        plt.scatter(centroid0[0], centroid0[1], color='white', label='Initial Centroid', marker='D', edgecolors='black')
        plt.scatter(centroidF[0], centroidF[1], color='purple', label='Final Centroid', marker='D', edgecolors='black')

        plt.quiver(
            bs_x[0], bs_y[0],
            phi_x, phi_y,
            angles='xy', scale_units='xy', scale=0.03, color='black', label='Beam Direction')

        plt.xlabel("X (m)")
        plt.ylabel("Y (m)")
        plt.legend(fontsize=9)
        plt.grid(True)
        plt.axis("equal")

        if nameSave != "":
          plt.savefig(nameSave, dpi=600)

        plt.show()

    def obs_norm(self, obs):

        obs_ = np.array(obs)

        min_p = self.powermin
        max_p = self.powermax
        min_t = self.tiltmin
        max_t = self.tiltmax
        min_phi = self.phimin
        max_phi = self.phimax
        min_RSRPmin = -130
        max_RSRPmin = -95
        min_RSRPavg = -121
        max_RSRPavg = -90
        min_delta = -15
        max_delta = 15

        obs_[0] = (obs_[0]-min_p)/(max_p-min_p)
        obs_[1] = (obs_[1]-min_t)/(max_t-min_t)
        obs_[2] = (obs_[2]-min_phi)/(max_phi-min_phi)
        obs_[3] = (obs_[3]-min_RSRPmin)/(max_RSRPmin-min_RSRPmin)
        obs_[4] = (obs_[4]-min_RSRPavg)/(max_RSRPavg-min_RSRPavg)
        obs_[5] = (obs_[5]-min_delta)/(max_delta-min_delta)
        obs_[6] = obs_[6]

        return obs_

"""# Training"""

dt = 1
t0 = 0
t_end = 60
num_samples = int((t_end-t0) / dt + 1)
timespan = np.linspace(t0, t_end, num_samples)

ue_num = 5
cue_num = 5
ue_names = [f"UE{i+1}" for i in range(ue_num)]
cue_names = [f"UE{i+1+ue_num}" for i in range(cue_num)]
ue = [UserEquipment(name) for name in ue_names]
cue = [UserEquipment(name) for name in cue_names]

env = NetworkEnvironment(ue, cue, t_0=t0, t_f = t_end, dt = dt)
obs_space = env.observation_space
n_actions = env.n_actions
agent = Agent(input_dims = obs_space, idx = 0, env=env, n_actions=n_actions)

n_games = 500


score_history = np.zeros(n_games)
start_time = time.time()

print("...TRAINING...")
for i in range(n_games):

    observation = env.reset()
    obs_normalized = env.obs_norm(observation)
    done = False
    step = 0
    std = 0.3 * np.exp(-i / (0.6 * n_games))


    while not done:
        action = agent.choose_action_training(obs_normalized, std)
        action = np.array(action)

        new_obs, reward, done, info = env.step(action)
        new_obs_normalized = env.obs_norm(new_obs)

        step+=1
        score_history[i] += reward

        agent.remember(obs_normalized, action, reward, new_obs_normalized, done)  # (s,a,r,s')
        agent.learn()

        obs_normalized = new_obs_normalized

    print(f'Ep. {i:3}   Score {score_history[i]:6.3f} --- [Power: {new_obs[0]:.3f}     Tilt: {new_obs[1]:.3f}     Phi: {new_obs[2]:.3f}     RSRP_min: {new_obs[3]:.3f}     RSRP_avg: {new_obs[4]:.3f}    DRSRP_min: {new_obs[5]:.3f}]')
end_time = time.time()

sim_time = end_time - start_time
print(f"\nSimulation time: {sim_time}")

# Plot Reward
plt.figure(figsize=(12, 6))
plt.plot(score_history)
plt.xlabel('Episodes')
plt.ylabel('Score')
plt.title('Reward evolution')
plt.grid(True)
plt.show()

"""# Evaluation"""

cue_pos = np.load("blue_users.npy")
bs_pos = np.load("BS_2_pos.npy")

ue_num = 20
cue_num = len(cue_pos)
ue_names = [f"UE{i+1}" for i in range(ue_num)]
cue_names = [f"UE{i+1+ue_num}" for i in range(cue_num)]

ue = [UserEquipment(name) for name in ue_names]
cue = [UserEquipment(name) for name in cue_names]

dt = 1
action_sampling = 0.1
t0 = 0
t_end = 100
num_samples = int((t_end-t0) / dt + 1)
timespan = np.linspace(t0, t_end*action_sampling, num_samples)
env = NetworkEnvironment(ue, cue, position=(bs_pos[0],bs_pos[1], 90), phi_0=180, t_0=t0, t_f = t_end, dt = dt)

obs_space = env.observation_space
n_actions = env.n_actions
agent = Agent(input_dims = obs_space, idx = 0, env=env, n_actions=n_actions)

seed = 146
observation, info = env.reset_evaluation(cue_pos, seed, 7)

done = False

power_signal = [observation[0]]
tilt_signal = [observation[1]]
phi_signal = [observation[2]]
RSRP_min = [observation[3]]
RSRP_avg = [observation[4]]
RSRP_delta = [observation[5]]

RSRP_initialUE = [info[0]]
RSRP_finalUE = [info[1]]

energy = [observation[0]]
u = []

score = 0
obs_normalized = env.obs_norm(observation)

i = 0
while not done:

  action = agent.choose_action_evaluation(obs_normalized)
  action = np.array(action)
  u.append(action)

  new_obs, reward, done, info = env.step_evaluation(action)

  score+=reward

  obs_normalized = env.obs_norm(new_obs)

  power_signal.append(new_obs[0])
  tilt_signal.append(new_obs[1])
  phi_signal.append(new_obs[2])
  RSRP_min.append(new_obs[3])
  RSRP_avg.append(new_obs[4])
  RSRP_delta.append(new_obs[5])
  energy.append(new_obs[0]+energy[-1])
  RSRP_initialUE.append(info[0])
  RSRP_finalUE.append(info[1])
  i+=1

print(f"[FINAL SCORE: {score:.2f}]")
print(f"Final Power: {new_obs[0]:.2f}W, Final Tilt: {new_obs[1]:.2f}°, Final Phi: {new_obs[2]:.2f}°\n")

"""# Load NN"""

agent.actor.load_weights('actor.weights.h5')
agent.target_actor.load_weights('target_actor.weights.h5')
agent.critic.load_weights('critic.weights.h5')
agent.target_critic.load_weights('target_critic.weights.h5')

with open('replay_buffer.pkl', 'rb') as f:
    agent.memory = pickle.load(f)